{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36fa12a4",
   "metadata": {},
   "source": [
    "# 张量\n",
    "简介\n",
    "几何代数中定义的张量是基于向量和矩阵的推广，比如我们可以将标量视为零阶张量，矢量可以视为一阶张量，矩阵就是二阶张量。\n",
    "    0维张量/标量 标量是一个数字\n",
    "    1维张量/向量 1维张量称为“向量”。\n",
    "    2维张量 2维张量称为矩阵\n",
    "    3维张量 公用数据存储在张量 时间序列数据 股价 文本数据 彩色图片(RGB)\n",
    "这里有一些存储在各种类型张量的公用数据集类型\n",
    "    3维=时间序列\n",
    "    4维=图像\n",
    "    5维=视频\n",
    "例子：一个图像可以用三个字段表示\n",
    "\n",
    "(width, height, channel) = 3D\n",
    "\n",
    "但是，在机器学习工作中，我们经常要处理不止一张图片或一篇文档——我们要处理一个集合。我们可能有10,000张郁金香的图片，这意味着，我们将用到4D张量：\n",
    "\n",
    "(sample_size, width, height, channel) = 4D\n",
    "\n",
    "在PyTorch中， torch.Tensor 是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现 Tensor 和NumPy的多维数组非常类似。然而，Tensor 提供GPU计算和自动求梯度等更多功能，这些使 Tensor 这一数据类型更加适合深度学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eaec828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入模块\n",
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1dcd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4256, 0.5607, 0.2223],\n",
      "        [0.2262, 0.8031, 0.8144],\n",
      "        [0.9208, 0.8809, 0.9964],\n",
      "        [0.2176, 0.6535, 0.8428]])\n"
     ]
    }
   ],
   "source": [
    "# 创建tensor，构建一个随机初始化的矩阵\n",
    "x = torch.rand(4,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37293f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 构建一个矩阵为全0，数据类型设置为long\n",
    "x = torch.zeros(4,3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea275e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.6000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# 直接构建张量\n",
    "x = torch.tensor([6.6, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f42f073c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[-1.6948, -0.0314,  1.8921],\n",
      "        [ 0.6539, -0.7992, -0.9246],\n",
      "        [-1.3707, -0.3535,  0.3868],\n",
      "        [-1.4615, -0.5193,  0.6136]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(4,3, dtype=torch.double) # 创建一个新的tensor，返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "x = torch.randn_like(x,dtype=torch.float)\n",
    "# 重置 数据类型\n",
    "print(x)\n",
    "# 结果会有一样的size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92df2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 获取它的维度信息\n",
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbe777",
   "metadata": {},
   "source": [
    "# 操作：加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4abeaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2949,  0.1528,  2.7512],\n",
      "        [ 1.6422,  0.1278, -0.2436],\n",
      "        [-0.4987,  0.2130,  0.6826],\n",
      "        [-1.0456,  0.2382,  1.5065]])\n",
      "tensor([[-1.2949,  0.1528,  2.7512],\n",
      "        [ 1.6422,  0.1278, -0.2436],\n",
      "        [-0.4987,  0.2130,  0.6826],\n",
      "        [-1.0456,  0.2382,  1.5065]])\n",
      "tensor([[-1.2949,  0.1528,  2.7512],\n",
      "        [ 1.6422,  0.1278, -0.2436],\n",
      "        [-0.4987,  0.2130,  0.6826],\n",
      "        [-1.0456,  0.2382,  1.5065]])\n",
      "tensor([[-1.2949,  0.1528,  2.7512],\n",
      "        [ 1.6422,  0.1278, -0.2436],\n",
      "        [-0.4987,  0.2130,  0.6826],\n",
      "        [-1.0456,  0.2382,  1.5065]])\n"
     ]
    }
   ],
   "source": [
    "# 方式1\n",
    "y = torch.rand(4, 3) \n",
    "print(x + y)\n",
    "\n",
    "# 方式2\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 方式3 提供一个输出 tensor 作为参数\n",
    "result = torch.empty(5, 3) \n",
    "torch.add(x, y, out=result) \n",
    "print(result)\n",
    "\n",
    "# 方式4 in-place\n",
    "y.add_(x) \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe9294",
   "metadata": {},
   "source": [
    "## 需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1b018c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3052, 2.9686, 4.8921])\n",
      "tensor([2.3052, 3.9686, 5.8921])\n",
      "tensor([2.3052, 3.9686, 5.8921])\n"
     ]
    }
   ],
   "source": [
    "# 取第二列\n",
    "print(x[0,:]) \n",
    "y = x[0,:]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4a737",
   "metadata": {},
   "source": [
    "# 改变张量大小\n",
    "想改变一个tensor的大小或者形状，可以使用torch.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08c09c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # -1是指这一维的维数由其他维度决定\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ed34cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3469, 2.5170, 2.5744, 3.4734],\n",
      "        [2.5887, 2.1250, 2.2133, 3.5821],\n",
      "        [2.3016, 1.8480, 1.9123, 2.1498],\n",
      "        [2.4869, 1.7523, 2.9804, 1.9380]])\n",
      "tensor([1.3469, 2.5170, 2.5744, 3.4734, 2.5887, 2.1250, 2.2133, 3.5821, 2.3016,\n",
      "        1.8480, 1.9123, 2.1498, 2.4869, 1.7523, 2.9804, 1.9380])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "注意 view() 返回的新tensor与源tensor共享内存(其实是同一个tensor)，也即更改其中的一个，另 外一个也会跟着改变。\n",
    "(顾名思义，view仅仅是改变了对这个张量的观察⻆度)\n",
    "'''\n",
    "x +=1\n",
    "print(x)\n",
    "print(y)# 也加了1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "136862ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., requires_grad=True)\n",
      "tensor(2., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# clone()深拷贝，clone 创造一个真正新的副本\n",
    "import torch\n",
    "\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = a.clone()\n",
    "a.data *= 3\n",
    "b += 1\n",
    "\n",
    "print(a)   # tensor(3., requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "750c173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7170])\n",
      "1.716983437538147\n"
     ]
    }
   ],
   "source": [
    "# 如果你有一个元素 tensor ，使用 .item() 来获得这个 value：\n",
    "x = torch.randn(1) \n",
    "print(x) \n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7870ee",
   "metadata": {},
   "source": [
    "# 广播机制\n",
    "当对两个形状不同的 Tensor 按元素运算时，可能会触发广播(broadcasting)机制：先适当复制元素使这两个 Tensor 形状相同后再按元素运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70808838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339ec53",
   "metadata": {},
   "source": [
    "由于 x 和 y 分别是1行2列和3行1列的矩阵，如果要计算 x + y ，那么 x 中第一行的2个元素被广播 (复制)到了第二行和第三行，⽽ y 中第⼀列的3个元素被广播(复制)到了第二列。如此，就可以对2 个3行2列的矩阵按元素相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ee7d4",
   "metadata": {},
   "source": [
    "# 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e96ba1",
   "metadata": {},
   "source": [
    "pyTorch 中，所有神经网络的核心是 autograd 包。autograd包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义 ( define-by-run ）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的。\n",
    "\n",
    "torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性。\n",
    "\n",
    "注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。\n",
    "\n",
    "要阻止一个张量被跟踪历史，可以调用.detach()方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。为了防止跟踪历史记录(和使用内存），可以将代码块包装在 with torch.no_grad(): 中。在评估模型时特别有用，因为模型可能具有 requires_grad = True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。\n",
    "\n",
    "还有一个类对于autograd的实现非常重要：Function。Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。\n",
    "\n",
    "如果需要计算导数，可以在 Tensor 上调用 .backward()。如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个gradient参数，该参数是形状匹配的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "721d0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个张量设置requires_grad=True用来追踪其计算\n",
    "import torch\n",
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f556880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x0000027A07EE6070>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntensor([[1., 1.],\\n        [1., 1.]], grad_fn=<PowBackward0>)\\ny是计算的结果，所以它有grad_fn属性\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "'''\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]], grad_fn=<PowBackward0>)\n",
    "y是计算的结果，所以它有grad_fn属性\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "888412d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>) tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 对y进行更多的操作\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac2a87fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000027A07E40580>\n"
     ]
    }
   ],
   "source": [
    "# 默认情况下 requires_grad = False\n",
    "a = torch.randn(2,2)\n",
    "a = ((a * 3)/(a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9c7d7",
   "metadata": {},
   "source": [
    "## 梯度\n",
    "\n",
    "现在开始进行反向传播，因为 out 是一个标量，因此out.backward()和 out.backward(torch.tensor(1.)) 等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edb3a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 输出导数 d(out)/dx\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9564dd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 注意：grad在反向传播过程中是累加的，这意味这每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零\n",
    "# 再来反向传播一次，注意grad是累加的2 out2 = x.sum()\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60979bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4186, -1.3713, -0.4784], requires_grad=True)\n",
      "tensor([ -428.6697, -1404.2080,  -489.9154], grad_fn=<MulBackward0>)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# 雅可比向量积的例子：\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i = i + 1\n",
    "print(y)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2ebd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# 在这种情况下，y 不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward：\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17601e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止 autograd 跟踪设置了.requires_grad=True的张量的历史记录。\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3138844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我么可以对 tensor.data 进行操作。\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c5616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
