{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a178523",
   "metadata": {},
   "source": [
    "# 损失函数\n",
    "\n",
    "一个好的训练离不开优质的负反馈，这里的损失函数就是模型的负反馈。\n",
    "所以在PyTorch中，损失函数是必不可少的。它是数据输入到模型当中，产生的结果与真实标签的评价指标，我们的模型可以按照损失函数的目标来做出改进。\n",
    "\n",
    "1. 二分类交叉熵损失函数\n",
    "2. 交叉熵损失函数\n",
    "3. L1损失函数\n",
    "4. MSE损失函数\n",
    "5. 平滑L1损失函数\n",
    "6. 目标泊松分布的损失函数\n",
    "7. KL散度\n",
    "8. MarginRankingloss\n",
    "9. 多标签边界损失函数\n",
    "10. 二分类损失函数\n",
    "11. 多分类的折页损失\n",
    "12. 三元组损失\n",
    "13. HingEmbeddingloss\n",
    "14. 余弦相似度\n",
    "15. CTC损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e80c41",
   "metadata": {},
   "source": [
    "# 二分类交叉熵损失函数\n",
    "\n",
    "功能：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。\n",
    "主要参数： \n",
    "weight:每个类别的loss设置权值\n",
    "\n",
    "size_average:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "\n",
    "reduce:数据类型为bool，为True时，loss的返回是标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9656fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "# 二分类交叉熵损失函数\n",
    "torch.nn.BCELoss(weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "# 功能：计算二分类任务时的交叉熵（Cross Entropy）函数。在二分类中，label是{0,1}。对于进入交叉熵函数的input为概率分布的形式。一般来说，input为sigmoid激活层的输出，或者softmax的输出。\n",
    "# 主要参数： \n",
    "# weight:每个类别的loss设置权值\n",
    "# size_average:数据为bool，为True时，返回的loss为平均值；为False时，返回的各样本的loss之和。\n",
    "# reduce:数据类型为bool，为True时，loss的返回是标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b678ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss损失函数的计算结果为 tensor(0.3991, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('BCELoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f567ef",
   "metadata": {},
   "source": [
    "## L1损失函数\n",
    "**功能：** 计算输出y和真实标签target之间的差值的绝对值。\n",
    "\n",
    "torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "我们需要知道的是，reduction参数决定了计算模式。有三种计算模式可选：none：逐个元素计算。 sum：所有元素求和，返回标量。 mean：加权平均，返回标量。 \n",
    "如果选择none，那么返回的结果是和输入元素相同尺寸的。默认计算方式是求平均。\n",
    "## 计算公式\n",
    "![title](img/L1损失函数.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f98b74be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1损失函数的计算结果为 tensor(1.3846, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('L1损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c43c8eb",
   "metadata": {},
   "source": [
    "# MSE损失函数\n",
    "**功能：**计算输出y和真实标签target之差的平方。\n",
    "\n",
    "和L1Loss一样，MSELoss损失函数中，\n",
    "reduction参数决定了计算模式。有三种计算模式可选；\n",
    "none：逐个元素计算；\n",
    "sum：所有元素求和，返回标量。默认计算方式是求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce61308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE损失函数的计算结果为 tensor(1.3051, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSE损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf8b03",
   "metadata": {},
   "source": [
    "# 平滑L1 (Smooth L1)损失函数\n",
    "torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n",
    "**功能**： L1的平滑输出，其功能是减轻离群点带来的影响\n",
    "\n",
    "reductio:n参数决定了计算模式。有三种计算模式可选：\n",
    "\n",
    "none：逐个元素计算。 \n",
    "\n",
    "sum：所有元素求和，返回标量。\n",
    "\n",
    "mean:默认计算方式是求平均。\n",
    "\n",
    "**提醒**： 之后的损失函数中，关于reduction 这个参数依旧会存在。所以，之后就不再单独说明。\n",
    "**计算公式如下 **\n",
    "![title](img/L1平滑.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9db714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothL1Loss损失函数的计算结果为 tensor(0.7501, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.SmoothL1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('SmoothL1Loss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d836e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1损失和平滑L1损失的对比\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "inputs = torch.linspace(-10, 10, steps=5000)\n",
    "target = torch.zeros_like(inputs)\n",
    "\n",
    "loss_f_smooth = nn.SmoothL1Loss(reduction='none')\n",
    "loss_smooth = loss_f_smooth(inputs, target)\n",
    "loss_f_l1 = nn.L1Loss(reduction='none')\n",
    "loss_l1 = loss_f_l1(inputs,target)\n",
    "\n",
    "plt.plot(inputs.numpy(), loss_smooth.numpy(), label='Smooth L1 Loss')\n",
    "plt.plot(inputs.numpy(), loss_l1, label='L1 loss')\n",
    "plt.xlabel('x_i - y_i')\n",
    "plt.ylabel('loss value')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284ad1e",
   "metadata": {},
   "source": [
    "# 目标泊松分布的负对数似然损失\n",
    "torch.nn.PoissonNLLLoss(log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n",
    "\n",
    "**主要参数**\n",
    "\n",
    "log_input：输入是否为对数形式，决定计算公式。\n",
    "\n",
    "full：计算所有 loss，默认为 False。\n",
    "\n",
    "eps：修正项，避免 input 0 为 时，log(input) 为 nan 的情况。\n",
    "\n",
    "数学公式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.PoissonNLLLoss()\n",
    "log_input = torch.randn(5, 2, requires_grad=True)\n",
    "target = torch.randn(5, 2)\n",
    "output = loss(log_input, target)\n",
    "output.backward()\n",
    "\n",
    "print('PoissonNLLLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605edd2",
   "metadata": {},
   "source": [
    "# KL散度\n",
    "torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "\n",
    "**功能**： 计算KL散度，也就是计算相对熵。用于连续分布的距离度量，并且对离散采用的连续输出空间分布进行回归通常很有用。\n",
    "\n",
    "**主要参数:**\n",
    "\n",
    "none：逐个元素计算。\n",
    "\n",
    "sum：所有元素求和，返回标量。\n",
    "\n",
    "mean：加权平均，返回标量。\n",
    "\n",
    "batchmean：batchsize 维度求平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea17037",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "loss = nn.KLDivLoss()\n",
    "output = loss(inputs,target)\n",
    "\n",
    "print('PoissonNLLLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8d58dd",
   "metadata": {},
   "source": [
    "# MarginRankingLoss\n",
    "torch.nn.MarginRankingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "功能： 计算两个向量之间的相似度，用于排序任务。该方法计算两组数据之间的差异。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f886a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "\n",
    "print('MarginRankingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d17592",
   "metadata": {},
   "source": [
    "#  多标签边界损失函数\n",
    "torch.nn.MultiLabelMarginLoss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "功能： 对于多标签分类问题计算损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.9, 0.2, 0.4, 0.8]])\n",
    "# for target y, only consider labels 3 and 0, not after label -1\n",
    "y = torch.LongTensor([[3, 0, -1, 1]])# 真实的分类是，第3类和第0类\n",
    "output = loss(x, y)\n",
    "\n",
    "print('MarginRankingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7d1e7",
   "metadata": {},
   "source": [
    "# 二分类损失函数\n",
    "torch.nn.SoftMarginLoss(size_average=None, reduce=None, reduction='mean')torch.nn.(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "功能： 二分类的 logistic 损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])  # 两个样本，两个神经元\n",
    "target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)  # 该 loss 为逐个神经元计算，需要为每个神经元单独设置标签\n",
    "\n",
    "loss_f = nn.SoftMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "\n",
    "print('SoftMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69dea3",
   "metadata": {},
   "source": [
    "#  多分类的折页损失\n",
    "torch.nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "**功能**： 计算多分类的折页损失\n",
    "\n",
    "**主要参数**:\n",
    "\n",
    "reduction：计算模式，可为 none/sum/mean。\n",
    "\n",
    "p：可选 1 或 2。\n",
    "\n",
    "weight：各类别的 loss 设置权值。\n",
    "\n",
    "margin：边界值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca51b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]]) \n",
    "target = torch.tensor([0, 1], dtype=torch.long) \n",
    "\n",
    "loss_f = nn.MultiMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "\n",
    "print('MultiMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f14ec0",
   "metadata": {},
   "source": [
    "# 三元组损失\n",
    "torch.nn.TripletMarginLoss(margin=1.0, p=2.0, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "**功能**： 计算三元组损失\n",
    "\n",
    "**三元组**: 这是一种数据的存储或者使用格式。<实体1，关系，实体2>。在项目中，也可以表示为< anchor, positive examples , negative examples>\n",
    "在这个损失函数中，我们希望去anchor的距离更接近positive examples，而远离negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c450b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "output = triplet_loss(anchor, positive, negative)\n",
    "output.backward()\n",
    "print('TripletMarginLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f2619",
   "metadata": {},
   "source": [
    "# HingEmbeddingLoss\n",
    "\n",
    "torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "**功能**： 对输出的embedding结果做Hing损失计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ab95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.HingeEmbeddingLoss()\n",
    "inputs = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "output = loss_f(inputs,target)\n",
    "\n",
    "print('HingEmbeddingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601b833",
   "metadata": {},
   "source": [
    "# 余弦相似度\n",
    "torch.nn.CosineEmbeddingLoss(margin=0.0, size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "**功能**： 对与两个向量做余弦相似度\n",
    "\n",
    "**主要参数:**\n",
    "\n",
    "reduction：计算模式，可为 none/sum/mean。\n",
    "\n",
    "margin：可取值[-1,1] ，推荐为[0,0.5] 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CosineEmbeddingLoss()\n",
    "inputs_1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\n",
    "inputs_2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\n",
    "target = torch.tensor([[1, -1]], dtype=torch.float)\n",
    "output = loss_f(inputs_1,inputs_2,target)\n",
    "\n",
    "print('CosineEmbeddingLoss损失函数的计算结果为',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06799bf",
   "metadata": {},
   "source": [
    "# CTC损失函数\n",
    "torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)\n",
    "\n",
    "**功能**： 用于解决时序类数据的分类\n",
    "\n",
    "计算连续时间序列和目标序列之间的损失。CTCLoss对输入和目标的可能排列的概率进行求和，产生一个损失值，这个损失值对每个输入节点来说是可分的。输入与目标的对齐方式被假定为 \"多对一\"，这就限制了目标序列的长度，使其必须是≤输入长度。\n",
    "\n",
    "**主要参数:**\n",
    "\n",
    "reduction：计算模式，可为 none/sum/mean。\n",
    "\n",
    "blank：blank label。\n",
    "\n",
    "zero_infinity：无穷大的值或梯度值为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b38dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target are to be padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "S = 30      # Target sequence length of longest target in batch (padding length)\n",
    "S_min = 10  # Minimum target length, for demonstration purposes\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Target are to be un-padded\n",
    "T = 50      # Input sequence length\n",
    "C = 20      # Number of classes (including blank)\n",
    "N = 16      # Batch size\n",
    "\n",
    "# Initialize random batch of input vectors, for *size = (T,N,C)\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "# Initialize random batch of targets (0 = blank, 1:C = classes)\n",
    "target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)\n",
    "target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "\n",
    "print('CTCLoss损失函数的计算结果为',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55905a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python bianbian_ML",
   "language": "python",
   "name": "bianbian_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
